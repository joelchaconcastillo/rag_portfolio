# ðŸ§  Joelâ€™s Assistant â€” Personalized RAG Chatbot

**Joelâ€™s Assistant** is a **Retrieval-Augmented Generation (RAG)** chatbot built with **LangChain**, **FastAPI**, and **Streamlit**.
Itâ€™s designed to provide **personalized, professional answers** for recruiters by leveraging both **local knowledge** and **online context** from [Joelâ€™s LinkedIn profile](https://www.linkedin.com/in/joel-chacon-castillo-351bb4194/).

---

## ðŸš€ Features

* ðŸ’¬ **Interactive Chat Interface** â€” Streamlit-based modern UI
* ðŸ§  **Retrieval-Augmented Generation** â€” Context-aware answers using embeddings and vector search
* ðŸ”— **LinkedIn Data Integration** â€” Uses professional profile as an online data source
* ðŸ’¾ **Local Knowledge Base** â€” Reads structured information from `/data/user_information`
* âš™ï¸ **Multiple Run Modes** â€” Streamlit, FastAPI (Uvicorn), or command line
* ðŸŽ¨ **Dark-Mode Chat Design** â€” Newest messages displayed on top

---

## ðŸ§° Project Structure

```
rag_portfolio/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __pycache__/                   # Compiled Python cache
â”‚   â”œâ”€â”€ config.py                      # Configuration utilities
â”‚   â”œâ”€â”€ core/                          # Main RAG components
â”‚   â”‚   â”œâ”€â”€ personalized_rag.py        # Core RAG pipeline
â”‚   â”‚   â”œâ”€â”€ indexer.py                 # Builds and updates vector database
â”‚   â”‚   â””â”€â”€ retriever.py               # Handles document retrieval
â”‚   â”œâ”€â”€ preprocessing/                 # Preprocessing scripts for data ingestion
â”‚   â””â”€â”€ main.py                        # FastAPI backend (Uvicorn entrypoint)
â”‚
â”œâ”€â”€ chroma_db/
â”‚   â””â”€â”€ chroma.sqlite3                 # Persistent Chroma vector store
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ user_information/              # Local knowledge base
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ streamlit_app.py               # Streamlit frontend (UI version)
â”‚
â”œâ”€â”€ render.yaml                        # Render deployment configuration
â”œâ”€â”€ requirements.txt                   # Project dependencies
â”œâ”€â”€ set_variables.sh                   # Environment variable setup script
â”œâ”€â”€ streamlit_app.py                   # Root Streamlit entry (for local dev)
â””â”€â”€ README.md
```

---

## ðŸ§© Tech Stack

| Component           | Description                                   |
| ------------------- | --------------------------------------------- |
| **Frontend**        | [Streamlit](https://streamlit.io)             |
| **Backend**         | [FastAPI](https://fastapi.tiangolo.com/)      |
| **Core Framework**  | [LangChain](https://www.langchain.com/)       |
| **Vector Database** | [ChromaDB](https://www.trychroma.com)         |
| **Embeddings**      | Gemini, HuggingFace, or OpenAI (configurable) |
| **Deployment**      | Render / Hugging Face Spaces / Local          |

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/joelchaconcastillo/rag_portfolio.git
cd rag_portfolio
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment Variables

If youâ€™re using external models like **Gemini** or **OpenAI**, create a `.env` file:

```bash
GEMINI_API_KEY=your_gemini_api_key
OPENAI_API_KEY=your_openai_api_key
```

You can also export them with:

```bash
source set_variables.sh
```

---

## â–¶ï¸ Running the Application

You can run **Joelâ€™s Assistant** in three different modes:

---

### ðŸ’» Option 1: Streamlit (Frontend UI)

This launches the **interactive chat interface**.

**Run from root:**

```bash
streamlit run streamlit_app.py
```

**Or run the version in `/frontend`:**

```bash
streamlit run frontend/streamlit_app.py
```

Then open your browser at:
ðŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

### âš¡ Option 2: FastAPI with Uvicorn (Backend API)

If you want to expose an API endpoint for integration (e.g., React, Postman, or external tools):

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

Access the FastAPI docs here:
ðŸ‘‰ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

**Example API call:**

```bash
curl -X POST "http://127.0.0.1:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "What are Joelâ€™s technical strengths?"}'
```

---

### ðŸ§® Option 3: Terminal Mode (Direct CLI)

To quickly test the RAG pipeline from the command line:

```bash
python -m app.core.personalized_rag
```

Or, if you have a test script (like `test.py`):

```bash
python test.py
```

**Example prompt:**

```
> What is Joelâ€™s professional background?
Answer: Joel ChacÃ³n Castillo is a software engineer specialized in AI, FastAPI, and cloud-based ML deployments...
```

---

## ðŸ§  How It Works

```
[User Question]
      â†“
 Streamlit / FastAPI
      â†“
Personalized_RAG (LangChain)
      â†“
[Retriever] â†’ [ChromaDB] â†’ [Documents / LinkedIn Data]
      â†“
[LLM Generator]
      â†“
[Final Answer]
```

* The assistant retrieves relevant chunks from your **local data** and **LinkedIn profile**
* Uses **vector embeddings** for semantic search
* Combines retrieved data with an **LLM** to generate a natural, context-rich response

---

## â˜ï¸ Deployment on Render

1. Push the repo to GitHub: [rag_portfolio](https://github.com/joelchaconcastillo/rag_portfolio)
2. Go to [Render.com](https://render.com/)
3. Create a **New Web Service**
4. Connect your GitHub repo
5. Choose the build command and start command:

**Build Command:**

```bash
pip install -r requirements.txt
```

**Start Command:**

```bash
streamlit run streamlit_app.py --server.port $PORT --server.address 0.0.0.0
```

6. Deploy ðŸŽ‰
   Render will automatically wake up your app when it receives traffic.

---

## ðŸ§ª Example Conversation

**User:**

> What projects has Joel recently worked on?

**Assistant:**

> Joel has developed RAG-based assistants integrating LangChain, FastAPI, and Streamlit for intelligent data retrieval and recruiter-focused prototypes.
> His work emphasizes deploying AI solutions with robust cloud integrations and scalable APIs.

---

## ðŸ“„ License

This project is licensed under the **MIT License** â€” free to use, modify, and distribute.

---

## ðŸ‘¤ Author

**Joel ChacÃ³n Castillo**
ðŸ’¼ [LinkedIn](https://www.linkedin.com/in/joel-chacon-castillo-351bb4194/)
ðŸ’» [GitHub Repository](https://github.com/joelchaconcastillo/rag_portfolio)





# Deploy FastAPI on Render

Use this repo as a template to deploy a Python [FastAPI](https://fastapi.tiangolo.com) service on Render.

See https://render.com/docs/deploy-fastapi or follow the steps below:

## Manual Steps

1. You may use this repository directly or [create your own repository from this template](https://github.com/render-examples/fastapi/generate) if you'd like to customize the code.
2. Create a new Web Service on Render.
3. Specify the URL to your new repository or this repository.
4. Render will automatically detect that you are deploying a Python service and use `pip` to download the dependencies.
5. Specify the following as the Start Command.

    ```shell
    uvicorn main:app --host 0.0.0.0 --port $PORT
    ```

6. Click Create Web Service.

Or simply click:

[![Deploy to Render](https://render.com/images/deploy-to-render-button.svg)](https://render.com/deploy?repo=https://github.com/joelchaconcastillo/rag_portfolio)

## Thanks

Thanks to [Harish](https://harishgarg.com) for the [inspiration to create a FastAPI quickstart for Render](https://twitter.com/harishkgarg/status/1435084018677010434) and for some sample code!
